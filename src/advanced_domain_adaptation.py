#!/usr/bin/env python3
"""
Advanced Domain Adaptation for 90%+ Cross-Dataset Performance

Implements state-of-the-art domain adaptation techniques:
1. Domain Adversarial Neural Networks (DANN)
2. Contrastive Domain Adaptation
3. Multi-scale Feature Alignment
4. Progressive Domain Transfer
5. Uncertainty-guided Adaptation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.autograd import Function
import logging

logger = logging.getLogger(__name__)

class GradientReversalFunction(Function):
    """Gradient Reversal Layer for Domain Adversarial Training"""
    
    @staticmethod
    def forward(ctx, x, lambda_):
        ctx.lambda_ = lambda_
        return x.view_as(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        output = grad_output.neg() * ctx.lambda_
        return output, None

class GradientReversalLayer(nn.Module):
    def __init__(self, lambda_=1.0):
        super().__init__()
        self.lambda_ = lambda_
    
    def forward(self, x):
        return GradientReversalFunction.apply(x, self.lambda_)

class DomainClassifier(nn.Module):
    """Domain discriminator for adversarial training"""
    
    def __init__(self, feature_dim=1280, hidden_dim=512):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim // 2, 2)  # Source vs Target domain
        )
    
    def forward(self, x):
        return self.classifier(x)

class ContrastiveLoss(nn.Module):
    """Contrastive loss for domain adaptation"""
    
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
    
    def forward(self, features_source, features_target, labels_source):
        # Normalize features
        features_source = F.normalize(features_source, dim=1)
        features_target = F.normalize(features_target, dim=1)
        
        # Compute similarity matrix
        similarity = torch.matmul(features_source, features_target.T) / self.temperature
        
        # Create positive pairs (same class)
        labels_source = labels_source.unsqueeze(1)
        labels_target = labels_source.T  # Assume same distribution for now
        
        positive_mask = (labels_source == labels_target).float()
        
        # Contrastive loss
        exp_sim = torch.exp(similarity)
        log_prob = similarity - torch.log(exp_sim.sum(dim=1, keepdim=True))
        
        loss = -(log_prob * positive_mask).sum() / positive_mask.sum()
        return loss

class AdvancedDomainAdaptationModel(nn.Module):
    """Advanced model with multiple domain adaptation techniques"""
    
    def __init__(self, backbone, num_classes, feature_dim=1280):
        super().__init__()
        self.backbone = backbone
        self.num_classes = num_classes
        self.feature_dim = feature_dim
        
        # Feature extractor (frozen backbone features)
        self.feature_extractor = backbone.backbone.features
        self.avgpool = backbone.backbone.avgpool
        
        # Multi-scale feature alignment
        self.feature_align = nn.Sequential(
            nn.Linear(feature_dim, feature_dim),
            nn.BatchNorm1d(feature_dim),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # Class classifier
        self.class_classifier = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
        
        # Domain classifier with gradient reversal
        self.gradient_reversal = GradientReversalLayer()
        self.domain_classifier = DomainClassifier(feature_dim)
        
        # Uncertainty estimation
        self.uncertainty_head = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
        
        # Contrastive loss
        self.contrastive_loss = ContrastiveLoss()
        
    def forward(self, x, domain_label=None, return_features=False):
        # Extract features
        features = self.feature_extractor(x)
        features = self.avgpool(features)
        features = torch.flatten(features, 1)
        
        # Feature alignment
        aligned_features = self.feature_align(features)
        
        # Class prediction
        class_logits = self.class_classifier(aligned_features)
        
        # Uncertainty estimation
        uncertainty = self.uncertainty_head(aligned_features)
        
        if return_features:
            return class_logits, aligned_features, uncertainty
        
        # Domain classification (for training)
        if domain_label is not None:
            reversed_features = self.gradient_reversal(aligned_features)
            domain_logits = self.domain_classifier(reversed_features)
            return class_logits, domain_logits, uncertainty
        
        return class_logits, uncertainty

class ProgressiveDomainTransfer:
    """Progressive domain transfer with curriculum learning"""
    
    def __init__(self, model, source_loader, target_loader, device):
        self.model = model
        self.source_loader = source_loader
        self.target_loader = target_loader
        self.device = device
        
        # Optimizers
        self.optimizer_main = torch.optim.Adam(
            list(model.feature_align.parameters()) + 
            list(model.class_classifier.parameters()),
            lr=1e-4, weight_decay=1e-5
        )
        
        self.optimizer_domain = torch.optim.Adam(
            model.domain_classifier.parameters(),
            lr=1e-4, weight_decay=1e-5
        )
        
        # Loss functions
        self.class_criterion = nn.CrossEntropyLoss()
        self.domain_criterion = nn.CrossEntropyLoss()
        
    def train_epoch(self, epoch, total_epochs):
        """Train one epoch with progressive adaptation"""
        self.model.train()
        
        # Progressive lambda for gradient reversal
        p = float(epoch) / total_epochs
        lambda_p = 2. / (1. + np.exp(-10 * p)) - 1
        self.model.gradient_reversal.lambda_ = lambda_p
        
        # Iterators
        source_iter = iter(self.source_loader)
        target_iter = iter(self.target_loader)
        
        total_loss = 0
        class_loss_total = 0
        domain_loss_total = 0
        contrastive_loss_total = 0
        
        num_batches = min(len(self.source_loader), len(self.target_loader))
        
        for batch_idx in range(num_batches):
            # Get source batch
            try:
                source_images, source_labels, _ = next(source_iter)
            except StopIteration:
                source_iter = iter(self.source_loader)
                source_images, source_labels, _ = next(source_iter)
            
            # Get target batch
            try:
                target_images, _, _ = next(target_iter)
            except StopIteration:
                target_iter = iter(self.target_loader)
                target_images, _, _ = next(target_iter)
            
            source_images = source_images.to(self.device)
            source_labels = source_labels.to(self.device)
            target_images = target_images.to(self.device)
            
            # Forward pass on source
            source_class_logits, source_features, source_uncertainty = self.model(
                source_images, return_features=True
            )
            
            # Forward pass on target
            target_class_logits, target_features, target_uncertainty = self.model(
                target_images, return_features=True
            )
            
            # Class loss (source only)
            class_loss = self.class_criterion(source_class_logits, source_labels)
            
            # Domain adversarial loss
            batch_size = source_images.size(0)
            domain_labels_source = torch.zeros(batch_size, dtype=torch.long).to(self.device)
            domain_labels_target = torch.ones(batch_size, dtype=torch.long).to(self.device)
            
            # Domain classification
            source_domain_logits = self.model.domain_classifier(
                self.model.gradient_reversal(source_features)
            )
            target_domain_logits = self.model.domain_classifier(
                self.model.gradient_reversal(target_features)
            )
            
            domain_loss = (
                self.domain_criterion(source_domain_logits, domain_labels_source) +
                self.domain_criterion(target_domain_logits, domain_labels_target)
            ) * 0.5
            
            # Contrastive loss for feature alignment
            contrastive_loss = self.model.contrastive_loss(
                source_features, target_features, source_labels
            )
            
            # Uncertainty-weighted loss
            uncertainty_weight = 1.0 - source_uncertainty.mean()
            
            # Total loss
            total_batch_loss = (
                class_loss * uncertainty_weight +
                domain_loss * lambda_p * 0.1 +
                contrastive_loss * 0.01
            )
            
            # Backward pass
            self.optimizer_main.zero_grad()
            self.optimizer_domain.zero_grad()
            
            total_batch_loss.backward()
            
            self.optimizer_main.step()
            self.optimizer_domain.step()
            
            # Accumulate losses
            total_loss += total_batch_loss.item()
            class_loss_total += class_loss.item()
            domain_loss_total += domain_loss.item()
            contrastive_loss_total += contrastive_loss.item()
        
        # Average losses
        avg_total_loss = total_loss / num_batches
        avg_class_loss = class_loss_total / num_batches
        avg_domain_loss = domain_loss_total / num_batches
        avg_contrastive_loss = contrastive_loss_total / num_batches
        
        logger.info(f"Epoch {epoch}: Lambda={lambda_p:.3f}")
        logger.info(f"  Total Loss: {avg_total_loss:.4f}")
        logger.info(f"  Class Loss: {avg_class_loss:.4f}")
        logger.info(f"  Domain Loss: {avg_domain_loss:.4f}")
        logger.info(f"  Contrastive Loss: {avg_contrastive_loss:.4f}")
        
        return {
            'total_loss': avg_total_loss,
            'class_loss': avg_class_loss,
            'domain_loss': avg_domain_loss,
            'contrastive_loss': avg_contrastive_loss,
            'lambda': lambda_p
        }

def create_advanced_domain_adaptation_model(backbone_model, num_classes):
    """Create advanced domain adaptation model"""
    return AdvancedDomainAdaptationModel(backbone_model, num_classes)

def train_with_domain_adaptation(source_loader, target_loader, val_loader, 
                                num_classes, num_epochs=20, device='cpu'):
    """Train model with advanced domain adaptation"""
    from src.efficientnet import EfficientNetB0Classifier
    
    logger.info("Creating advanced domain adaptation model...")
    
    # Create backbone
    backbone = EfficientNetB0Classifier(num_classes=num_classes, pretrained=True)
    
    # Create advanced model
    model = create_advanced_domain_adaptation_model(backbone, num_classes)
    model = model.to(device)
    
    # Create progressive trainer
    trainer = ProgressiveDomainTransfer(model, source_loader, target_loader, device)
    
    # Training loop
    best_val_acc = 0.0
    training_history = []
    
    for epoch in range(num_epochs):
        logger.info(f"\n=== Epoch {epoch+1}/{num_epochs} ===")
        
        # Train epoch
        epoch_results = trainer.train_epoch(epoch, num_epochs)
        
        # Validation
        val_acc = evaluate_domain_adaptation_model(model, val_loader, device)
        epoch_results['val_accuracy'] = val_acc
        
        training_history.append(epoch_results)
        
        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'models/best_domain_adaptation_model.pth')
            logger.info(f"âœ… New best validation accuracy: {best_val_acc:.4f}")
        
        logger.info(f"Validation Accuracy: {val_acc:.4f}")
    
    logger.info(f"\nTraining completed! Best validation accuracy: {best_val_acc:.4f}")
    
    return model, training_history

def evaluate_domain_adaptation_model(model, test_loader, device):
    """Evaluate domain adaptation model"""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels, _ in test_loader:
            images, labels = images.to(device), labels.to(device)
            
            outputs, uncertainty = model(images)
            _, predicted = torch.max(outputs.data, 1)
            
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    accuracy = correct / total
    return accuracy